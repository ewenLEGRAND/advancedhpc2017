** Title : Report Lab 1 **

** Student : Ewen Legrand **

** Mail : elegrand@enssat.fr **

******************************************************

Mission
*******

	Convert existing labwork 1 sequential CPU code to parallel using OpenMP. The 
original code is Labwork::labwork1_CPU() and I have to modifie the code : Labwork:
:labwork1_OpenMP().

        In order to measure the difference between the two code, I will use an
existing profiling class Timer which will show the speedup difference.

How ?
*****

	I will use OpenMP command like #pragma omp directive [clause]. In this case,
the ddirective will be "parallel" and the clause will be "for" because of the for loop.

	So the OpenMP comand, #pragma omp parallel for, will be positioned just before 
the 2 loops. I verified that the variables are effectively declared inside the parallelized
region.        

Speedup
*******
	
	for "#pragma omp parallel for" command,

		USTH ICT Master 2017, Advanced Programming for HPC.
		Warming up...
		Starting labwork 1
		labwork 1 CPU ellapsed 2859.3ms
		labwork 1 OpenMP ellapsed 785.3ms
		labwork 1 ellapsed 785.3ms

	I obtain approximatly the same time measured during the Labwork 0 for the labwork 1
CPU compute : 2859.3 ms. And for the labwork 1 OpenMP compute time, I measure 785.3 ms which 
is much better than CPU compute time. There a factor of 3.6 between the two measures so I can
say that, thanks to OpenMP and the parallel compute, the image compute is 3.6 faster than the
classic sequentiel CPU compute.

	I will change now the number of threads (bloc size variation). To do
that, I will use the omp_set_num_threads() function available in OpenMP 
library. I choose to change the number of threads between 2 and 12.

2   => 1791.6 ms
4   => 1126.3 ms
6   => 816.2 ms
8   => 953.8 ms
10  => 904.4 ms
12  => 787.5 ms
14  => 917.7 ms
16  => 838.3 ms
18  => 792.5 ms


^
|    x
|
|
|
|        x
|                x
|                    x       x
|            x                   x
|                        x           x
|
|
0==============================================>
     2   4   6   8   10  12  14  16  18 


	Now, I will test the static configuration vs the dynamic configuration. For that,
I will use the following command thanks to OpenMP :
 
		#pragma omp for schedule(static)
		#pragma omp for schedule(dynamic)

	I tested with the 2 best block size, 6 and 12, in order to compare the static configuration
and the dynamic configuration.

Block size	 Static		Dynamic

    6           792.6 ms        839.7 ms
    12          768.1 ms        769.4 ms 


	I can see that the static configuration is the best configuration for this program. Static
is the default schedule as shown above. Upon entering the loop, each thread independently decides
which chunk of the loop they will process.

	In the dynamic schedule, there is no predictable order in which the loop items are assigned
to different threads. Each thread asks the OpenMP runtime library for an iteration number, then
andles it, then asks for next, and so on. This is most useful when used in conjunction with the
ordered clause, or when the different iterations in the loop may take different time to execute.

	I can also specify the chunk size with the following command :

		#pragma omp for schedule(dynamic, chunkSize)

For example, I will try to run the program with a chunk size of 3. In this example, each thread
asks for an iteration number, executes 3 iterations of the loop, then asks for another, ect...
The last chunk may be smaller than 3, though.

	Result with a chunk size of 3 and a block size of 12 (best config) : 842.4 ms

Of course, that's a constraint for the compute of the program.
